{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we apply gradient descent for an SDP problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the problem identically to that in Problem 9 in HW1 and HW2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Construct the anchor points\n",
    "a1 = np.array([1.0, 0.0])\n",
    "a2 = np.array([-1.0, 0.0])\n",
    "a3 = np.array([0.0, 2.0])\n",
    "\n",
    "\n",
    "# Construct the vectors of our triangle\n",
    "v1 = a1 - a2\n",
    "v2 = a3 - a2\n",
    "\n",
    "# Function to generate a random point inside or outside of \n",
    "# our convex hull. \n",
    "def point_generator(outside=False):\n",
    "    # Compute a random point within the quadrilateral\n",
    "    rand_point = a2 + (np.random.random() * v1 + np.random.random() * v2)\n",
    "    \n",
    "    # Compute the barycentric coordinates to determine whether the point is inside\n",
    "    # the triangle, or outside of it\n",
    "    alpha = ((a2[1] - a3[1])*(rand_point[0] - a3[0]) + (a3[0] - a2[0])* \\\n",
    "            (rand_point[1] - a3[1])) / ((a2[1] - a3[1])*(a1[0] - a3[0]) + \\\n",
    "            (a3[0] - a2[0])*(a1[1] - a3[1]))\n",
    "    beta = ((a3[1] - a1[1])*(rand_point[0] - a3[0]) + (a1[0] - a3[0])* \\\n",
    "            (rand_point[1] - a3[1])) / ((a2[1] - a3[1])*(a1[0] - a3[0]) + \\\n",
    "            (a3[0] - a2[0])*(a1[1] - a3[1]))\n",
    "    gamma = 1.0 - alpha - beta\n",
    "\n",
    "    # If each barycentric coordinate is greater than 0, the point is inside\n",
    "    if alpha > 0.0 and beta > 0.0 and gamma > 0.0:\n",
    "        # If we want the point outside\n",
    "        if outside:\n",
    "            # Repeat the procedure\n",
    "            return point_generator(outside)\n",
    "        else:\n",
    "            # Return our random point inside the triangle\n",
    "            return rand_point\n",
    "    else: # The point is outside\n",
    "        if outside:\n",
    "            # Return our random point outside the triangle\n",
    "            return rand_point\n",
    "        else:\n",
    "            # Repeat the procedure\n",
    "            return point_generator()\n",
    "        \n",
    "# Construct our random point\n",
    "p = point_generator()\n",
    "\n",
    "# The inequality values of our constraints in the SDP problem\n",
    "b = np.array([1,1,2, np.linalg.norm(p - a1), \\\n",
    "     np.linalg.norm(p - a2),                 \\\n",
    "     np.linalg.norm(p - a3)])\n",
    "\n",
    "#Starting point X\n",
    "X_temp = np.random.rand(3,3) * 3\n",
    "X = np.matmul(X_temp.transpose(), X_temp)\n",
    "\n",
    "#The constraints of our SDP problem\n",
    "A_ = [np.outer(np.array([1, 0, 0]), np.array([1, 0, 0])), \\\n",
    "     np.outer(np.array([0, 1, 0]), np.array([0, 1, 0])),  \\\n",
    "     np.outer(np.array([1, 1, 0]), np.array([1, 1, 0])),  \\\n",
    "     np.outer(np.append(a1, -1), np.append(a1, -1)),      \\\n",
    "     np.outer(np.append(a2, -1), np.append(a2, -1)),      \\\n",
    "     np.outer(np.append(a3, -1), np.append(a3, -1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The objective barrier function\n",
    "def phi(a, x, b, mu):\n",
    "    return (0.5 * np.dot((A_X(a, x) - b).transpose(), A_X(a, x) - b)) \\\n",
    "            - mu * np.log(np.linalg.det(x))\n",
    "\n",
    "#The gradient of the objective barrier function in 8b\n",
    "def dphi(a, x, b, mu):\n",
    "    y = A_X(a, x) - b\n",
    "\n",
    "    gradf = np.zeros((3, 3))\n",
    "    for ind, A in enumerate(a):\n",
    "        gradf = gradf + y[ind] * A\n",
    "    return gradf - mu*np.linalg.pinv(x)\n",
    "\n",
    "#The updated gradient of the object barrier function in 8c\n",
    "def ddphi(a, x, b, mu):\n",
    "    y = A_X(a, x) - b\n",
    "    gradf = np.zeros((3, 3))\n",
    "    for ind, A in enumerate(a):\n",
    "        gradf = gradf + y[ind] * A\n",
    "    return np.matmul(np.matmul(x, gradf), x) - mu*x\n",
    "\n",
    "#Performs the operation AX as enumerated in the assignment\n",
    "def A_X(a, x):\n",
    "    element_wise = [np.multiply(A, x) for A in a]\n",
    "    return np.array([A.sum() for A in element_wise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performs steepest descent\n",
    "def steepest_descent(op, dop, a, xin, b, mu, niter):\n",
    "    x = np.copy(xin)\n",
    "    for i in range(0, niter):\n",
    "        grad = dop(a,x,b,mu)\n",
    "        alpha = 1.\n",
    "        \n",
    "        #Here, we perform the backtracking line search\n",
    "        #but we also want the smallest eigenvalue of x^(k+1)\n",
    "        #to be at least half of the smallest eigenvalue of\n",
    "        #x^k\n",
    "        e_val_new, t = np.linalg.eig(x - alpha*grad)\n",
    "        e_val_old, t = np.linalg.eig(x)\n",
    "        ratio = np.amin(e_val_new)/np.amin(e_val_old)\n",
    "        \n",
    "        #Perform backtracking\n",
    "        while((phi(a,x - alpha*grad, b, mu) > phi(a, x, b, mu)) \\\n",
    "              or ratio < 0.5):\n",
    "            alpha *= 0.8\n",
    "            e_val_new, t = np.linalg.eig(x - alpha*grad)\n",
    "            e_val_old, t = np.linalg.eig(x)\n",
    "            ratio = np.amin(e_val_new)/np.amin(e_val_old)\n",
    "        x -= alpha * grad\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The point value is:  [-0.14318903  1.07715705]\n",
      "The value of optimal solution is [ 10.67273789  12.39781623]\n"
     ]
    }
   ],
   "source": [
    "x_new = steepest_descent(phi, dphi, A_, X, b, 0, 300);\n",
    "print \"The point value is: \", p\n",
    "print \"The value of optimal solution is\", x_new[2, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We note that the recovered solution is incorrect. This is because our solution has no barrier $\\mu$ and we are using the ill-conditioned form of gradient descent. Furthermore, the convergence is very slow from the multiple steps that the backtracking search performs (since we have that $\\mu = 0$. We try the gradient descent update again with the new gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The point value recovered is:  [-0.14318903  1.07715705]\n",
      "The value of optimal solution is [-0.03817452  0.6715733 ]\n"
     ]
    }
   ],
   "source": [
    "x_new = steepest_descent(phi, ddphi, A_, X, b, 0, 300);\n",
    "print \"The point value recovered is: \", p\n",
    "print \"The value of optimal solution is\", x_new[2, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we do not recover the exact position correctly, we are in the neighborhood (at least in the convex hull) of the correct solution with the correct magnitudes. Although our convergence is much slower, we move close to the expected value. Now, we try new value of $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The point value is:  [-0.14318903  1.07715705]\n",
      "The value of optimal solution is [  6.5728207   10.20246336]\n"
     ]
    }
   ],
   "source": [
    "x_new = steepest_descent(phi, dphi, A_, X, b, 1e-4, 300);\n",
    "print \"The point value is: \", p\n",
    "print \"The value of optimal solution is\", x_new[2, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the recovered solution is much better than the previous attempt with $\\mu = 0$, we still note that we are still quite off from the correct solution. This is because we are penalizing getting close to 0. However, now the execution time is much quicker than before since we do not have to perform as much backtracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The point value is:  [-0.14318903  1.07715705]\n",
      "The value of optimal solution is [-0.03826969  0.67134881]\n"
     ]
    }
   ],
   "source": [
    "x_new = steepest_descent(phi, ddphi, A_, X, b, 1e-4, 300);\n",
    "print \"The point value is: \", p\n",
    "print \"The value of optimal solution is\", x_new[2, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our solution is not quite what we desire, but we see that the execution time is quicker with the new value of $\\mu$ and we recover a solution that is relatively close. This is the best method out of the four methods of gradient descent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
